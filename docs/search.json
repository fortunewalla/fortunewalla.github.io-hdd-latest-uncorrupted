[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "L'Œuvre",
    "section": "",
    "text": "Explore product management, data science, & the metaverse.\n\n\n\n\n\n\n“Turn yourself not away from three best things: Good Thought, Good Word, and Good Deed.” ~ Zarathushtra\nThe website content is divided into four main types:\n\nProjects: Longer term and more complex work involving code, and other software websites.\nArticles: Shorter term work involving coding and analysis and opinion. Reviews would come under articles.\nGuides: Longer term and frequently updated notes for learning topics.\nMisc: Random stuff that does not fit anything formal.\n\nThe content categories are further divided into four main areas:\n\nProduct Management: Planning, creating, designing, managing, & selling products. UI/UX design, technology, business & startups.\nData Science: Data science, analytics, visualization, machine learning & artificial intelligence.\nMetaverse: Includes Augmented, Virtual, Mixed, Extended Reality hardware, software, code & business\nSoftware: Usage, coding, setup, scripts, algorithsm, & recommendations.\n\nClick here to view Portfolio of work"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "L'Œuvre",
    "section": "",
    "text": "Explore product management, data science, & the metaverse.\n\n\n\n\n\n\n\n\n\nAbout this website.\nCurrent Work & Projects:\n\nReview of paper “A comprehensive overview of software product management challenges”\nJira Product Discovery Workflows\nExploring Inclusive Design for new HCI through the metaverse\n\nBelow is a list of all work done and ongoing projects.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nPost With Code\n\n\nFeb 17, 2023\n\n\n\n\nWelcome To My Blog\n\n\nFeb 17, 2023\n\n\n\n\nSQL & Database Administration Setup & Tips\n\n\nMar 26, 2023\n\n\n\n\nSQL Guide.\n\n\nFeb 18, 2020\n\n\n\n\nSome questions to ask entrepreneurs\n\n\nFeb 11, 2016\n\n\n\n\nExtracting initial parameters from an existing Holt‐Winter forecasting model\n\n\nNov 29, 2015\n\n\n\n\nForecasting with seasonal trends at BLAYK restaurant\n\n\nNov 24, 2015\n\n\n\n\nChoice of moving average or exponential smoothing for a particular product profile\n\n\nNov 23, 2015\n\n\n\n\nForecasting for Sugar Bon‐Bon Cereals\n\n\nNov 23, 2015\n\n\n\n\nExponential smoothing models at TrainMax Systems\n\n\nNov 23, 2015\n\n\n\n\nEvaluation of stationary demand models\n\n\nNov 20, 2015\n\n\n\n\nAnalysis of three black‐box type demand forecasting models\n\n\nNov 19, 2015\n\n\n\n\nSuitability of stationary demand models for forecasting\n\n\nNov 18, 2015\n\n\n\n\nPerformance characteristics of forecasting models\n\n\nNov 18, 2015\n\n\n\n\nImproving the naïve model forecast using cumulative period model\n\n\nNov 18, 2015\n\n\n\n\nHow do I to get started in & learn ‘data science’, ‘data analytics’?\n\n\nFeb 21, 2014\n\n\n\n\nThe best options to study for SAS Certified Base Programmer for SAS 9 Credential\n\n\nFeb 21, 2014\n\n\n\n\nCombinatorial analysis & calculations using SAS functions – fact(), perm() and comb()\n\n\nMar 7, 2014\n\n\n\n\nHow to find out and use the number of observations in a given SAS data set\n\n\nMar 16, 2014\n\n\n\n\nReferring to a SAS data set with its full filesystem path\n\n\nApr 4, 2014\n\n\n\n\nSoftware choices to implement a remote database client/server network setup\n\n\nMay 8, 2014\n\n\n\n\nProject to implement remote PHP web client/ MSSQL database server network setup\n\n\nMay 16, 2014\n\n\n\n\nConnect to and query Microsoft SQL Server using SAS/ACCESS Interface to ODBC\n\n\nMay 18, 2014\n\n\n\n\nWhat is ‘data science’ / data analytics? - Yet another opinion\n\n\nFeb 21, 2014\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/01-whatisdatascience/index.html",
    "href": "posts/01-whatisdatascience/index.html",
    "title": "What is ‘data science’ / data analytics? - Yet another opinion",
    "section": "",
    "text": "Created on Friday, February 21st, 2014 at 6:52 am\nIn the last twenty years of the Information Age, computer networks & the Internet have allowed us to gather & store information from a wide variety of devices (industrial/medical equipment, photos/audio/video sensors, ATMs, credit/debit cards, mobiles & computers, etc..) & platforms (retailing, telecom, banking & insurance , pharmaceutical, security, social media, etc..).\nThe information being collected is increasing as more people & businesses use computers & the Internet as a primary means for conducting business, social & monetary transactions. Technologies such as 3G/4G data networks, mobile computing devices & cloud based computing systems have accelerated this trend.\n\n“The value of data is no longer in how much of it you have. In the new regime, the value is in how quickly and how effectively can the data be reduced, explored, manipulated and managed.”\nUsama Fayyad – President & CEO of digiMine, Inc. [1]\n\nQuestions to think about:\n\nCan the increasing amount of information be collected, stored & managed in a consistent manner for easy access to others?\nCan information from different sources be integrated & linked to create meaningful relationships between them?\nCan information systems be built to easily get answers to characteristics of stored data?\nCan we detect underlying patterns in the data & “mine” the data to reveal patterns of behavior that will be insightful & profitable in terms of business/research before it loses its value?\nCan analytic processes/models be built to allow prediction of future outcomes/behavior from existing data more quickly than the competition?\nCan it be ensured that business/research returns will be greater than the investment cost of the data collection & analytics system?\n\n\nIt takes brilliance to ask the right questions, at the right time in history. The value of a Big Data resource is that a good analyst can start to see connections between different types of data, and this may prompt the analyst to determine whether there is a way to describe these connections in terms of general relationships among the data objects [2]\n\nIt is important to realize that most of these problems have been discussed [1] & studied in research journals & other industry publications for the last 30-40 years under various labels such as “Business Intelligence”, “Knowledge Discovery”, “Data Mining”, “Decision Science”, “Statistical Learning”, “Predictive Modeling” “Machine Learning”, “Business Forecasting”, etc… Essentially, it is the coming together of data analysis techniques, large scale computing & domain knowledge.\nNow in the present time, all these are under the labels of “Big Data” [2] “Data Science” & “Data Analytics”. The big change now is in the commoditization of technologies where all these techniques can be applied in a cost effective manner & almost in real-time.\nWhile it is not realistically possible for a single person to perform all the above tasks, a new practitioner has emerged who has the relevant knowledge in statistical & data mining techniques, computing & programming techniques as well as domain knowledge of the business/research problem.\nThe main skill required (besides the usual technical knowledge) is curiosity about what patterns exist or can be “mined” from stored data sources as well what can be predicted from it, the ability to experiment with new methods to get new insights & explanations just like a scientist would. Perhaps that is why they call people in this field “data scientists”!\nMy own equation for this “emerging” field would be:\n\nData Science/Data Analytics =\nDomain knowledge of the business/research problem +\nMathematical formulation of business/research problem into a statistical model +\nProgramming the statistical model into software code +\nBusiness/Research analysis of the statistical output\n\nReferences\n[1] HAMPARSUM BOZDOGAN (ed.) Statistical data mining and knowledge discovery (2003) Chapman & Hall/CRC\n[2] JULES J BERMAN Principles of big data: preparing, sharing, and analyzing complex information (2013) Morgan Kaufmann"
  },
  {
    "objectID": "posts/02-howdoigetstarted/index.html",
    "href": "posts/02-howdoigetstarted/index.html",
    "title": "How do I to get started in & learn ‘data science’, ‘data analytics’?",
    "section": "",
    "text": "Created on Friday, February 21st, 2014 at 6:57 am\nThis is an open ended question that has created a lot of discussion on the Web. There are no right answers or approaches. It all depends on what you are interested in & want to achieve. If you choose the self study route, this would be my personal approach.\n1) Decide your domain of interest: The fields of “data analytics”, “data science” are very vast in their scope to learn everything. So although the general statistical & analytic principles are the same for all fields, it is best to find your industry of interest (say pharmaceutical, econometrics, finance, social & bio sciences, human resources, actuarial sciences, marketing, energy forecasting, predictive modeling, business statistical analysis, medical sciences, etc..) & develop domain specific knowledge for your interests. This will allow you to focus your learning & effort. Trying to master too many domains might get you confused & mentally drained in the long run.\n2) Application or technical side: This is a broad generalization. Application side is where the understanding & application is more important than the implementation of the code or the software system. Typically for students of business, social sciences, pharmaceutical, etc…\nUsually understanding & framing the research/business problem required to be solved precedes analysis. The important thing is to know the business impact of your analysis. For example, by changing the values of the variables or doing a “WHAT…IF” analysis, one should be able to interpret the change in output in terms of how it addresses the research/business problem that is required to be solved. This type of skill comes only with having the right domain knowledge.\nTechnical side usually interests students of mathematics, computer science, engineering & fields where focus is on developing new techniques, software & improving existing ones. The fields of data mining & “Big Data” are pretty technical in terms of the mathematical & programming knowledge required. This includes algorithms & equations for data mining, machine learning, pattern recognition, text processing. Database design using “Big Data” technologies like Hadoop, NOSQL, Hive, MongoDb, PIG, MapReduce etc… Also most existing databases like MS SQL Server & Oracle have data mining features.\nAs technical conferences & statistical journals discuss the latest techniques & methods in terms of mathematics, it would be best to get the mathematical background (usually calculus 1 & 2, linear algebra & probability at undergraduate level) as quickly as you can. This will not only enable you to understand but also express yourself in terms of mathematics.\nThe ability to understand the research/business problem & convert it into mathematical form & then choose or create an appropriate algorithmic method, that is relatively fast & with minimum error, for the specific software system are skills you should aim to acquire.\n3) Books & reading material: The books & material you read from should match your level of expertise & also must be based on the software you plan to use. Your best option is to use academic websites, publisher websites & book review websites like Amazon.com to know the contents & the subject matter of the books. There are specialized statistical books for students of social sciences, marketing, computer science, pharmaceutical etc… Also books that teach data analytics/data science/statistics using particular software (such as R, SAS, SPSS, STATA, MS Excel, Python & many more).\nBuy textbooks on the following criteria:\n\nBased on your mathematical level i.e. based on advanced math like calculus or simple math like algebra.\nThat teaches & uses the software you plan to learn with. Using R, SAS, SPSS, STATA, MS Excel, etc….\nThat deal with the techniques & methods for your domain of interest i.e. finance, marketing, pharmaceutical, biostatistics etc…\n\n4) Get the software running: Being familiar with the various functions & features of the software is almost as important as learning statistical theory. You want to be productive & not waste time looking up help/documentation all the time.\nThere are software specifically for statistical analysis. Some are suited for certain domains & industries. The hyperlink below shows a list of them.\nhttp://www.amstat.org/careers/statisticalsoftware.cfm\nThe R project software is the ideal to begin learning with. It is extensive & has many user submitted packages for almost every kind of statistical analysis. It is available for free (as in gratis).\nSince R is distributed under the GPL software license you might need to be familiar with the licensing issues of the various R packages especially if you plan to use R commercially & your code also contains other proprietary code using restrictive licenses.\nMicrosoft Excel is a good option as most office/college computers have machines running MS Windows & MS Office. It has many add-ins (e.g. XLMiner, neuroXL, Oracle Spreadsheet Add-In, Perfringens Predictor Excel Add-in, ADAPA Add-in for Microsoft® Office Excel®, RExcel, DataMinerXL, SAS Add-in for Rapid Predictive Modeler, Palisade Neuraltools Add-in, 11Ants Model Builder Microsoft Excel Add-in, etc.. ) available for analytics & data mining. It can also be used to interact with the data mining features of MS SQL Server.\nCommercial statistical software like SAS, IBM SPSS & STATA are used widely in industry & academia. Those in academia should be able to get an academic license to access & use SAS/SPSS/STATA on their personal computers. SAS also offers an SAS OnDemand service to access SAS through the Internet for a fee.\n5) Programming: Since each software package has its own programming environment. Learning a general programming course at undergraduate level will help you understand programming principles that each software uses. Those technically inclined would do well to do a course in computer algorithms & database design.\nKnowing SQL is important since most of the data you will analyze or process will reside in a database system like MySQL, MS SQL Server, Oracle etc…\nA major part of analytics & data science is modifying existing data into a particular format (especially dates, currency, telephone, number formats) for processing. Every software has built-in features for checking errors & missing values, replacing, searching, sorting, filtering & extracting text from a larger dataset. Text processing tools like grep, perl, and python are also used.\nIt is good idea, although not necessary, to get the basic certifications for commercial software like SAS, SPSS. The exams allow you to brush up your skills & your clients or the company would be somewhat assured of your competency.\nLast but not least, it is best to explore websites & join academic/industry communities specific to your area of research or choice.\nSummary:\n\nDecide on your area of interest & domain.\nDo you prefer to be on the technical/programming side or be application/business oriented?\nGet the right books & study material for your academic level & area of interest.\nDecide on the appropriate software used by your industry.\nLearn programming & algorithms.\nExplore the Internet for websites & join communities specific to your needs."
  },
  {
    "objectID": "posts/03-thebestwaytostudyforsas/index.html",
    "href": "posts/03-thebestwaytostudyforsas/index.html",
    "title": "The best options to study for SAS Certified Base Programmer for SAS 9 Credential",
    "section": "",
    "text": "Created on Friday, February 21st, 2014 at 6:59 am\nIf SAS software will be a part of your career then get certified by passing the SAS Base Programming Exam from SAS Institute. It is designed to test your knowledge in writing SAS programs to access & manage data to perform queries and analyses. It also creates a good impression to your client / company.\nhttp://support.sas.com/certify/creds/bp.html\nOption 1: If you can afford it or have your company/institution sponsor/subsidize you, then the two courses offered by the SAS Institute is the best way to learn the Base SAS system & programming.\nSAS Programming 1: Essentials http://support.sas.com/edu/schedules.html?id=277&ctry=US\nSAS Programming 2: Data Manipulation Techniques http://support.sas.com/edu/schedules.html?id=278&ctry=US\nIt is a highly structured course with code examples, exercises & practice questions that will thoroughly teach you the basic concepts of SAS software & Base SAS programming.\nOption 2: The next best option is to purchase the following book\nBase SAS Programmer Certification 3rd edition\nhttp://www.amazon.com/SAS-Certification-Prep-Guide-Programming/dp/1607649241/ref=sr_1_1?ie=UTF8\nIt is a condensed version of the full course & hence is the best option for those that cannot afford the full course. Use it with the Base SAS software available at your academic institution or company & practice all the example code. You can also buy access to SAS OnDemand to use SAS software through the Internet to practice for the exam.\nOption 3: Join an institute that offer training in SAS & statistical analysis. They usually frame their SAS course syllabus & material from the official courses & have computer labs with access to Base SAS software. The advantage here is having an instructor who can give you feedback on your progress & clear your doubts & queries regarding the exam.\nPreparation: You can be ready with 4-5 months of preparation if you follow any of the above options. Modifying existing code & using different procedures / keywords to achieve the same result is best way to understand how SAS works. It requires time & practice as SAS software is vast & comprehensive.\nSAS Institute also offers a practice exam that you can purchase to test yourself before attempting the actual exam.\nIt should be understood that in addition to all this, there is an abundant amount of material on the Web to help you learn Base SAS programming & prepare for the certification credential exam. All the best!"
  },
  {
    "objectID": "posts/04-combinatorialfunctionsas/index.html",
    "href": "posts/04-combinatorialfunctionsas/index.html",
    "title": "Combinatorial analysis & calculations using SAS functions – fact(), perm() and comb()",
    "section": "",
    "text": "Created on Friday, March 7th, 2014 at 5:56 am\nSAS provides several kinds of functions for doing combinatorial analysis and calculations. Three basic ones will be demonstrated here. fact(),perm() and comb(). All three functions return a missing value for the arguments they cannot compute.\nfact(n) is the function for calculating the factorial n! of any non-negative number n.\n275  data _null_;\n276  a=fact(-3);\n277  b=fact(0);\n278  c=fact(9);\n279  d=fact(170);*170! is the max calculable by this particular computer.;\n280  e=fact(171);\n281  f=fact(1000);\n282  put _all_;\n283  run;\nThe output is given by:\nNOTE: Invalid argument to function FACT at line 276 column 3.\nNOTE: Invalid argument to function FACT at line 280 column 3.\nNOTE: Invalid argument to function FACT at line 281 column 3.\na=. b=1 c=362880 d=7.257416E306 e=. f=. _ERROR_=1 _N_=1\na=. b=1 c=362880 d=7.257416E306 e=. f=. _ERROR_=1 _N_=1\nNOTE: Mathematical operations could not be performed at the following places. The results of the\n      operations have been set to missing values.\n      Each place is given by: (Number of times) at (Line):(Column).\n      1 at 276:3   1 at 280:3   1 at 281:3\nNOTE: DATA statement used (Total process time):\nSimilarly for permutation of n objects taken r at a time (where n&gt;r), we have the perm(n,r). A single argument in the perm() function will calculate the factorial of the argument.\n384  data _null_;\n385  a=perm(3,3);\n386  b=perm(2,5);\n387  c=perm(5,2);\n388  d=perm(5,4);\n389  e=perm(1660,170);\n390  f=perm(4);\n391  put _all_;\n392  run;\nThe output is given by\nNOTE: Argument 2 to function PERM at line 386 column 3 is invalid.\nNOTE: Invalid argument to function PERM at line 389 column 3.\na=6 b=. c=20 d=120 e=. f=24 _ERROR_=1 _N_=1\na=6 b=. c=20 d=120 e=. f=24 _ERROR_=1 _N_=1\nNOTE: Mathematical operations could not be performed at the following places. The results of the\n      operations have been set to missing values.\n      Each place is given by: (Number of times) at (Line):(Column).\n      1 at 386:3   1 at 389:3\nNOTE: DATA statement used (Total process time):\nSimilarly for combination of n objects taken r at a time (where n&gt;r), we have the comb(n,r). The comb() function requires two arguments.\n433  data _null_;\n434  a=comb(3,3);\n435  b=comb(2,5);\n436  c=comb(5,2);\n437  d=comb(5,4);\n438  e=comb(9960,170);\n439  f=comb(1,0);\n440  put _all_;\n441  run;\nThe output is given by:\nNOTE: Argument 2 to function COMB at line 435 column 3 is invalid.\nNOTE: Invalid argument to function COMB at line 438 column 3.\na=1 b=. c=10 d=5 e=. f=1 _ERROR_=1 _N_=1\na=1 b=. c=10 d=5 e=. f=1 _ERROR_=1 _N_=1\nNOTE: Mathematical operations could not be performed at the following places.\n      The results of the operations have been set to missing values.\n      Each place is given by: (Number of times) at (Line):(Column).\n      1 at 435:3   1 at 438:3\nWe have the logarithmic (natural) counterparts of the above three functions i.e. lfact(), lperm() and lcomb() The output is given below.\n452  data _null_;\n453  a=lfact(10);\n454  b=lperm(10,5);\n455  c=lcomb(10,5);\n456  put _all_;\n457  run;\n\na=15.104412573 b=10.31692083 c=5.5294290875 _ERROR_=0 _N_=1\nNOTE: DATA statement used (Total process time):\n      real time           0.00 seconds\n      cpu time            0.00 seconds"
  },
  {
    "objectID": "posts/05-howtofindnoofobssas/index.html",
    "href": "posts/05-howtofindnoofobssas/index.html",
    "title": "How to find out and use the number of observations in a given SAS data set",
    "section": "",
    "text": "Created on Sunday, March 16th, 2014 at 5:43 am\nIf one is in a situation where they have to know the number of observations in a particular SAS data set or use it for further calculations, SAS allows many ways of doing so. Let us look at a few quick ways using the DATA step. SAS documentation also includes a SAS Macro approach which will not be replicated here.\nMethod 1: The most straightforward way is to load the set in a _null_ DATA step.\n data _null_;\n set dataset3obs;\n run;\nThe output is given by\n NOTE: There were 3 observations read from the data set WORK.DATASET3OBS.\n NOTE: DATA statement used (Total process time):\n real time 0.00 seconds\n cpu time 0.00 seconds\nMethod 2: Another way to extract & print is to output the NOBS= value, but since the DATA step loops to read all observations, the put statement is executed at every iteration.\n data _null_;\n set dataset3obs nobs=nobs;\n put nobs;\n run;\nThe output is given by\n 3\n 3\n 3\n NOTE: There were 3 observations read from the data set WORK.DATASET3OBS.\n NOTE: DATA statement used (Total process time):\n real time 0.00 seconds\n cpu time 0.00 seconds\nMethod 3: One way to overcome this & print a single value is to output the number of observations either at the beginning or at the end of DATA step.\n data _null_;\n set dataset3obs nobs=nobs;\n if _n_=1 then put nobs;\n run;\nThe output is given by\n 3\n NOTE: There were 3 observations read from the data set WORK.DATASET3OBS.\n NOTE: DATA statement used (Total process time):\n real time 0.00 seconds\n cpu time 0.00 seconds\nMethod 4: Output at the end of the data loop.\n data _null_;\n set dataset3obs nobs=nobs end=last;\n if last then put nobs;\n run;\nThe output is given by\n 3\n NOTE: There were 3 observations read from the data set WORK.DATASET3OBS.\n NOTE: DATA statement used (Total process time):\n real time 0.00 seconds\n cpu time 0.00 seconds\nMethod 5: One can even extract the value before the SET statement, as SAS loads it in the descriptor portion during the compilation phase.\n data _null_;\n set dataset3obs nobs=nobs;\n if _n_=1 then put nobs;\n run;\nThe output is given by\n 3\n NOTE: There were 3 observations read from the data set WORK.DATASET3OBS.\n NOTE: DATA statement used (Total process time):\n real time 0.00 seconds\n cpu time 0.00 seconds\nThere are many other methods but these are simple for just a quick lookup."
  },
  {
    "objectID": "posts/06-referringsasdatasetfullpath/index.html",
    "href": "posts/06-referringsasdatasetfullpath/index.html",
    "title": "Referring to a SAS data set with its full filesystem path",
    "section": "",
    "text": "Posted on Friday, April 4th, 2014 at 3:14 am\nThe standard way of referring to a SAS data set is through its library reference. i.e. LIBREF.DATASETNAME. However SAS also allows accessing the data set using the full filesystem path. Although not very useful, it does show the versatility that SAS allows the user. It can be used in situations where the data set needs to be accessed without defining a library name reference to access it.\nproc print data=sasuser.admit;\nrun;\nUsing the full filesystem path we get:\nproc print data='C:\\Documents and Settings\\sasuser\\My Documents\\My SAS Files\\9.1\\admit.sas7bdat'; \nrun;"
  },
  {
    "objectID": "posts/07-softwarechoicesremotedb/index.html",
    "href": "posts/07-softwarechoicesremotedb/index.html",
    "title": "Software choices to implement a remote database client/server network setup",
    "section": "",
    "text": "Posted on Thursday, May 8th, 2014 at 2:25 am\nInteracting with a database on a network from statistical/reporting software like MS Excel, MS Access, SAS, R, Python, etc… is a great way to learn how data might be retrieved, analysed & stored remotely. Web scripting languages like ASPX, PHP also allow remote database interactions for data analysis using Web based applications.\nHowever, a local database with local software using only individual local files pretty much provides the same experience as using them on a network. But the aim here is to simulate a corporate/research environment where all the software & data resources are spread throughout the network. The aim is not to become an expert in the technologies but to know just enough to use them to do data science/analytics.\nHence the philosophy is to use minimum hardware/software resources to be able to study & learn quickly and efficiently. The problem is that each person has a different configuration of client/server and hardware/software components & usually one person’s solution will not necessarily work for the other. The choices available to you will be different from the choices presented here.\nServer decision process:\n\nServer database: MS SQL Server (MSSQL) vs. Mysql Database (mysql)\nDecision: MSSQL\nReason: MSSQL has built-in data mining through SQL Server Analysis Services (SSAS). Also MSSQL2008 offers more data mining options when compared to MSSQL2005. There is more information online about MSSQL data mining features than about mysql data mining features. Furthermore, MS Excel can perform data mining directly on spreadsheets using SSAS via an add-in\nServer OS: Windows XP (winxp) vs. Windows Vista/7 (win7)\n\nDecision: win7\n\nReason: win7 has similar features to Windows Server 2008 & handles networking better than winxp. Winxp would be sufficient but one needs to install additional software components such as dotnet packages, VC++ run-time packages, deal with permissions/networking issues etc…\nOnce you have the server & software setup for networking & remote access, almost any hardware/software combination can be used as a client.\nClient decision process:\n\nClient OS: Windows vs. GNU/Linux\n\nDecision: Windows\nReason: MS Excel and MS Access are used widely in the business/research world. Hence Windows XP was chosen. Also MS Excel supports many add-ins for statistical analysis, data mining & visualization. Although software such as R, Python, WEKA etc… would integrate better with GNU/Linux.\n\nClient web server: MS Internet Information Services (IIS) vs. Apache Web Server (apache).\n\nDecision: IIS\n\nReason: A student of data science/analytics does not need advanced features of a web server. A basic version of IIS comes built-in with winxp. I am guessing it is enough for the purpose of learning. Apache is robust but requires more configuration than IIS.\n\nClient web language: ASPX vs. PHP\n\nDecision:PHP\n\nReason: PHP is easy to configure for IIS & light on system resources. ASPX would be the preferred choice for use with IIS and MSSQL, but requires more configuration & systems resources for Visual Studio 2010 development environment. Students can look at the PHPStats project https://github.com/mcordingley/PHPStats for using statistical functions to do data analysis on the web.\n\nOS for Web Server: Client OS vs. Server OS\n\nDecision: Client OS\n\nReason: Decision to install IIS/PHP on Client OS was primarily done for two reasons i) To isolate the database from any instability that might result from the Web application environment ii) To simulate an “Internet” where the database & web server reside on different machines.\n\nMisc. client software: MS Excel, MS Access, R, Python. It is advisable to try the database access features of many software for learning & practice.\nSummary:\nFinal server setup: SQL Server 2008 on Windows 7.\nFinal client setup: PHP using IIS on Windows XP.\nThe main idea is to simulate a corporate/research data science/analytics environment where data & software resources are spread throughout the network. The design choices are made to use minimum resources. This enables students to learn the basics efficiently without having to worry about the advanced features. The implementation of these choices will be discussed in another post."
  },
  {
    "objectID": "posts/08-projecttoimplementremotedb/index.html",
    "href": "posts/08-projecttoimplementremotedb/index.html",
    "title": "Project to implement remote PHP web client/ MSSQL database server network setup",
    "section": "",
    "text": "Posted on Friday, May 16th, 2014 at 2:43 pm\nFor students learning data science/analytics, accessing data sets stored from databases in remote servers is a necessary skill. People in corporations/institutions have Systems Administrators to set up most of the interfaces for such purposes. However knowing how such a process is setup is a good knowledge. Here PHP is used to demonstrate database access.\nAlthough this project is not related to data science/analytics, it will give a good hands-on experience of client/server networking. These are mere guidelines and not details as specific options & settings will depend on the version of the software & components.\nPre-requisites: Fairly good understanding of computer hardware especially networking, Windows operating system, drivers & software installation. Hardware/Networking\nPhysical: Atleast two computers with Ethernet/WiFi capability and a wireless/wired router.\nFinal Setup: Ability to create Workgroups & shared folders. Also ability to ping & telnet the remote computers/servers. Server Setup\nThere are many versions & editions MSSQL depending on user requirements (The Developer Edition is best for learning & academic use). For data mining features, SQL Server Analysis Services(SSAS) must be installed. To install OLTP/DW AdventureWorks sample databases, enable FILESTREAM and make sure the SQL Full-text Filter Daemon Launcher service is running.\n\nIf you have the space, do a full install of every feature. It will make it easier to troubleshoot problems.\nCreate a MSSQL user exclusively for remote access. http://technet.microsoft.com/en-us/library/aa337545.aspx\nAllow remote connection to MSSQL. http://technet.microsoft.com/en-us/library/ms191464.aspx\nEnable Shared Memory, TCP/IP, pipes http://technet.microsoft.com/en-us/library/ms181035.aspx\nIn the Windows Firewall, allow incoming/outgoing connections for port 1433 (default MSSQL port.) http://blogs.msdn.com/b/walzenbach/archive/2010/04/14/how-to-enable-remote-connections-in-sql-server-2008.aspx\n\nFinal Setup: From the Command Prompt in the client, execute ping dbpcip and telnet dbpcip 1433 where dbpcip is the IP address of MSSQL machine. If you can connect with both the commands, the client/server networking is probably working. Although accessing the datebase & tables finally depends on the permissions assigned to the remote user. Client Setup\nKeep these details in mind before starting installation. 1) Some of the types of PHP functions for access to MSSQL. This article only discusses mssql_() and sqldrv_().\n`mssql_()` Supported by PHP 5.2 (php52) and lower. MSSQL Driver for PHP (SSDPHP) not required\n`sqldrv_()` Supported by PHP 5.3 (php53) and higher. SSDPHP required.\n`pdo_sqlsrv_()` Supported by php52 and higher. SSDPHP required.\n`pdo_odbc_()` Supported by php51 and higher. SSDPHP not required.\n\nSQL Server Native Client (SSNC): This is needed by PHP to access MSSQL Server. SSNC2008: installs on winxp SSNC2012: does not install on winxp. php52: requires atleast SSNC2008 php53: requires atleast SSNC2012\nProblem: php53 onwards requires SSNC2012, but SSNC2012 does not work on winxp. Also php53 does not support mssql_() functions.\n\nSolution: SSNC2008 works on winxp. Use php 5.2.13 to 5.2.17 as php52 requires SSNC2008. Also php 5.2.x supports mssql_()functions and sqldrv_() functions if you install the SSDPHP 2.0. Client software\n\nInternet Information Services (IIS): Official Documentation: https://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/iiiisin2.mspx?mfr=true\n\nStep-by-step illustrated guide: Follow until step 10 http://www.wikihow.com/Configure-IIS-for-Windows-XP-Pro\nYou can test if it is working by opening the http://localhost/ address on the web browser. You should also test it on a remote computer using http://clientip/\nFinal Setup: Windows Version: XP IIS Version: 5.1\n\nFastCGI:\n\nFastCGI helps IIS work better with PHP. Install the one which is compatible with your version of IIS & OS. Official source: http://www.iis.net/downloads/microsoft/fastcgi-for-iis\nFinal Setup:fcgisetup_1.5_rtw_x86.msi\n\nPHP installation:\n\n\nSteps to installing PHP with IIS are explained in &lt;www.php.net/manual/en/install.windows.iis6.php&gt;\n5.2 and lower mssql_()documentation http://www.php.net/manual/en/book.mssql.php\n5.3 and higher sqlsrv_() documentation http://www.php.net/manual/en/book.sqlsrv.php\n5.2 and higher pdo_sqlsrv() documentation http://www.php.net/manual/en/ref.pdo-sqlsrv.php\n5.1 and higher pdo_odbc() documentation http://www.php.net/manual/en/ref.pdo-odbc.php\n\nFinal Setup: php-5.2.17-nts-Win32-VC6-x86.msi\n\nMSSQL Driver for PHP:\n\nOfficial Documentation: http://technet.microsoft.com/en-us/library/dn425064%28v=sql.10%29.aspx\nInstallation help: http://www.iis.net/learn/application-frameworks/install-and-configure-php-on-iis/install-the-sql-server-driver-for-php\nFinal Setup: SQLSRV20.exe\n\nSQL Server Native Client:\n\nUse this page to decide what components are needed for your setup. http://msdn.microsoft.com/en-us/library/cc296170%28SQL.105%29.aspx\nFinal Setup: sqlncli2k8r2x86.msi\n\nOverview: The entire process can be very broadly summarised as:\n\nIIS -&gt; FastCGI -&gt; PHP 5.3 -&gt; sqlsrv_() -&gt; SSDPHP 3.0 -&gt; SSNC -&gt; MSSQL IIS -&gt; FastCGI -&gt; PHP 5.2 -&gt; sqlsrv_() -&gt; SSDPHP 2.0 -&gt; SSNC -&gt; MSSQL IIS -&gt; FastCGI -&gt; PHP 5.2 -&gt; mssql_()-&gt; No SSDPHP -&gt; SSNC -&gt; MSSQL Testing the final setup:\n\nPHP System Information Test: If PHP is properly installed, the following code should execute:\n\nSOURCE CODE:\n&lt;?php echo phpinfo(); ?&gt;\nYou should get PHP system information which includes information about msql_() and sqlsrv_() extensions. Only relevant partial output shown below.\nOUTPUT:\n\n    \n        Registered PHP Streams \n            php, file, data, http, ftp, compress.zlib, compress.bzip2, \n        https, ftps, zip, sqlsrv   \n    \n\n\n\n\ncgi-fcgi\n\n\n    \n        Directive\n            Local Value\n            Master Value\n    \n\n    \n        cgi.check_shebang_line\n            1\n            1\n    \n\n    \n        cgi.fix_pathinfo\n            1\n            1\n    \n\n    \n        cgi.force_redirect\n            0\n            0\n    \n\n    \n        cgi.nph\n            0\n            0\n    \n\n    \n        cgi.redirect_status_env\n            no value\n            no value\n    \n\n    \n        cgi.rfc2616_headers\n            0\n            0\n    \n\n    \n        fastcgi.impersonate\n            1\n            1\n    \n\n    \n        fastcgi.logging\n            0\n            0\n    \n\n\n\nmsql\n\n\n    \n        MSQL Support \n            enabled \n    \n\n    \n        Allow Persistent Links \n            yes \n    \n\n    \n        Persistent Links \n            0/unlimited \n    \n\n    \n        Total Links \n            0/unlimited \n    \n\n\n\nmssql\n\n\n    \n        MSSQL Support\n            enabled\n    \n\n    \n        Active Persistent Links \n            0 \n    \n\n    \n        Active Links \n            0 \n    \n\n    \n        Library version \n            7.0 \n    \n\n\n\n\n    \n        Directive\n            Local Value\n            Master Value\n    \n\n    \n        mssql.allow_persistent\n            On\n            On\n    \n\n    \n        mssql.batchsize\n            0\n            0\n    \n\n    \n        mssql.compatability_mode\n            Off\n            Off\n    \n\n    \n        mssql.connect_timeout\n            5\n            5\n    \n\n    \n        mssql.datetimeconvert\n            On\n            On\n    \n\n    \n        mssql.max_links\n            Unlimited\n            Unlimited\n    \n\n    \n        mssql.max_persistent\n            Unlimited\n            Unlimited\n    \n\n    \n        mssql.max_procs\n            Unlimited\n            Unlimited\n    \n\n    \n        mssql.min_error_severity\n            10\n            10\n    \n\n    \n        mssql.min_message_severity\n            10\n            10\n    \n\n    \n        mssql.secure_connection\n            Off\n            Off\n    \n\n    \n        mssql.textlimit\n            Server default\n            Server default\n    \n\n    \n        mssql.textsize\n            Server default\n            Server default\n    \n\n    \n        mssql.timeout\n            60\n            60\n    \n\n\n\npdo_sqlsrv\n\n\n    \n        pdo_sqlsrv support\n            enabled\n    \n\n\n\n\n    \n        Directive\n            Local Value\n            Master Value\n    \n\n    \n        pdo_sqlsrv.log_severity\n            0\n            0\n    \n\n\n\nsqlsrv\n\n\n    \n        sqlsrv support\n            enabled\n    \n\n\n\n\n    \n        Directive\n            Local Value\n            Master Value\n    \n\n    \n        sqlsrv.LogSeverity\n            0\n            0\n    \n\n    \n        sqlsrv.LogSubsystems\n            0\n            0\n    \n\n    \n        sqlsrv.WarningsReturnAsErrors\n            On\n            On\n    \n\n\nRemote MSSQL Access Test: If phpinfo() indicates that all the drivers & extensions are installed properly, then test to see if the database server & client are properly registered.\n\nSOURCE CODE:\n\n&lt;?php\n$serverName = \"dbpcip, 1433\"; //serverName\\instanceName, portNumber (default is 1433)\n$connectionInfo = array( \"Database\"=&gt;\"master\", \"UID\"=&gt;\"userName\", \"PWD\"=&gt;\"passWord\");\n$conn = sqlsrv_connect( $serverName, $connectionInfo);\n\nif( $conn ) {\n     echo \"Connection established.\n\";\n}else{\n     echo \"Connection could not be established.\n\";\n     die( print_r( sqlsrv_errors(), true));\n}\n\nif( $client_info = sqlsrv_client_info( $conn)) {\n    foreach( $client_info as $key =&gt; $value) {\n        echo $key.\": \".$value.\"\n\";\n    }\n} else {\n    echo \"Error in retrieving client info.\n\";\n}\n\n\n$server_info = sqlsrv_server_info( $conn);\nif( $server_info )\n{\n    foreach( $server_info as $key =&gt; $value) {\n       echo $key.\": \".$value.\"\n\";\n    }\n} else {\n      die( print_r( sqlsrv_errors(), true));\n}\n?&gt;\nOUTPUT:\nConnection established.\n\n\n    \n        Parameter\n            Value\n    \n\n    \n        DriverDllName:\n            sqlncli10.dll\n    \n\n    \n        DriverODBCVer:\n            03.52\n    \n\n    \n        DriverVer:\n            10.50.1600\n    \n\n        \n        ExtensionVer:\n            2.0.1802.200\n    \n\n        \n        SQLServerVersion:\n            10.50.1600\n    \n\n        \n        SQLServerName:\n            DBPC\n\nTest of SQL Query: Once a connection is established, the final test is to see whether query execution is possible or not. This is a quick crude code. But the fact that it retrieves data from master.dbo.spt_monitor table shows the setup & the connection works.\n\nSOURCE CODE:\n&lt;?php\n$serverName = \"dbpcip, 1433\"; //serverName\\instanceName, portNumber (default is 1433)\n$connectionInfo = array( \"Database\"=&gt;\"master\", \"UID\"=&gt;\"userName\", \"PWD\"=&gt;\"passWord\");\n$conn = sqlsrv_connect( $serverName, $connectionInfo);\n\nerror_reporting(-1);\n\n\nif( $conn ) {\n     echo \"Connection established.\n\";\n}else{\n     echo \"Connection could not be established.\n\";\n     die( print_r( sqlsrv_errors(), true));\n}\n/* SQL Query */\n$sql=\"select * from dbo.spt_monitor\";\n$results = sqlsrv_query( $conn, $sql );\nif( $results === false) {\n    die( print_r( sqlsrv_errors(), true) );\n}\n    echo \"MSSQL master.dbo.spt_monitor TABLE\n\";\n        echo \"\n            &lt;table border=1&gt;\n            &lt;tr&gt;\n                &lt;th&gt;cpu_busy&lt;/th&gt;\n                &lt;th&gt;io_busy&lt;/th&gt;\n                &lt;th&gt;idle&lt;/th&gt;\n                &lt;th&gt;pack_received&lt;/th&gt;\n                &lt;th&gt;pack_sent&lt;/th&gt;\n                &lt;th&gt;connections&lt;/th&gt;\n                &lt;th&gt;pack_errors&lt;/th&gt;\n                &lt;th&gt;total_read&lt;/th&gt;\n                &lt;th&gt;total_write&lt;/th&gt;\n                &lt;th&gt;total_errors&lt;/th&gt;\n             &lt;/tr&gt;\";\n    while ($row = sqlsrv_fetch_array($results))\n    {\n                $cpu_busy=$row[1];\n                $io_busy=$row[2];\n                $idle=$row[3];\n                $pack_received=$row[4];\n                $pack_sent=$row[5];\n                $connections=$row[6];\n                $pack_errors=$row[7];\n                $total_read=$row[8];\n                $total_write=$row[9];\n                $total_errors=$row[10];\n             \n    echo \"\n            &lt;tr&gt;\n                &lt;td&gt;$cpu_busy&lt;/td&gt;;\n                &lt;td&gt;$io_busy&lt;/td&gt;;\n                &lt;td&gt;$idle&lt;/td&gt;;\n                &lt;td&gt;$pack_received&lt;/td&gt;;\n                &lt;td&gt;$pack_sent&lt;/td&gt;;\n                &lt;td&gt;$connections&lt;/td&gt;;\n                &lt;td&gt;$pack_errors&lt;/td&gt;;\n                &lt;td&gt;$total_read&lt;/td&gt;;\n                &lt;td&gt;$total_write&lt;/td&gt;;\n                &lt;td&gt;$total_errors&lt;/td&gt;;\n            &lt;/tr&gt;\";\n    }\n    echo \"&lt;/table&gt;\";\n    ?&gt;\nOUTPUT:\nConnection established.\n\nMSSQL master.dbo.spt_monitor TABLE\n\n; ; ; ; ; ; ; ; ; ;\n\n            \n\n                \n                    cpu_busy\n                        io_busy\n                        idle\n                        pack_received\n                        pack_sent\n                        connections\n                        pack_errors\n                        total_read\n                        total_write\n                        total_errors\n                \n\n                \n                    9\n                        7\n                        792\n                        28\n                        28\n                        14\n                        0\n                        0\n                        0\n                        0\n                \n\nIncorrect SSDPHP version:\n\nProblem: If you use php53/SSDPHP 3.0 code but use SSNC2008 instead of SSNC 2012, you get an error message.\nCould not connect. Array ( [0] =&gt; Array ( [0] =&gt; IMSSP [SQLSTATE] =&gt; IMSSP [1] =&gt; -49 [code] =&gt; -49 [2] =&gt; This extension requires the Microsoft SQL Server 2012 Native Client. Access the following URL to download the Microsoft SQL Server 2012 Native Client ODBC driver for x86: http://go.microsoft.com/fwlink/?LinkId=163712 [message] =&gt; This extension requires the Microsoft SQL Server 2012 Native Client. Access the following URL to download the Microsoft SQL Server 2012 Native Client ODBC driver for x86: http://go.microsoft.com/fwlink/?LinkId=163712 ) [1] =&gt; Array ( [0] =&gt; IM002 [SQLSTATE] =&gt; IM002 [1] =&gt; 0 [code] =&gt; 0 [2] =&gt; [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified [message] =&gt; [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified ) )\nSolution: Install php52/SSDPHP 2.0 & SSNC2008\nSummary:\nAlthough this project takes a lot of time to setup, troubleshoot & tweak all the settings, the end result is a client/server setup one can experiment with. Once the database server is setup properly, various types of software can be configured to retrieve/store data. PHP can be used to create web applications for doing data analysis using MSSQL.\nTo access data from MSSQL using php52, mssql_() functions is used. But php53 onwards uses only sqldrv_() functions to access MSSQL. Hence it is better to learn sqldrv_() functions. However knowing how to use the older functions will help when dealing with legacy systems having php52."
  },
  {
    "objectID": "posts/09-connectquerymssqlsasinterfaceodbc/index.html",
    "href": "posts/09-connectquerymssqlsasinterfaceodbc/index.html",
    "title": "Connect to and query Microsoft SQL Server using SAS/ACCESS Interface to ODBC",
    "section": "",
    "text": "Created on Sunday, May 18th, 2014 at 3:24 am\nOne of the important skills in SAS is being able to connect & query a local or remote database, conduct data analysis in SAS & write the new information back to the database. SAS provides access to a variety of databases as well as different ways of doing so. In this article, access to Microsoft SQL Server (MSSQL) using SAS/ACCESS ODBC interface is explained. If you are using SAS at an institute or organization, your Systems Administrator would have setup everything. The steps in this article might vary depending on the software configuration."
  },
  {
    "objectID": "posts/09-connectquerymssqlsasinterfaceodbc/index.html#summary",
    "href": "posts/09-connectquerymssqlsasinterfaceodbc/index.html#summary",
    "title": "Connect to and query Microsoft SQL Server using SAS/ACCESS Interface to ODBC",
    "section": "Summary:",
    "text": "Summary:\nThe SAS/ACCESS interface component is used to connect to various kinds of databases. Three main ways of connecting SAS with MSSQL are using SAS/ACCESS interface for MS SQL Server, SAS/ACCESS interface for OLE DB & SAS/ACCESS interface for ODBC. SAS allows data retrieval/storage using both LIBNAME statement and direct SQL Pass-Through statement method.\nWhat is shown above is very elementary code just to get the process started. Students learning data science/analytics should try to use different combinations of SAS methods & SAS interfaces to retrieve data from a database, manipulate it using SAS/SQL & write the processed data sets back to the database."
  },
  {
    "objectID": "posts/09-connectquerymssqlsasinterfaceodbc/index.html#references",
    "href": "posts/09-connectquerymssqlsasinterfaceodbc/index.html#references",
    "title": "Connect to and query Microsoft SQL Server using SAS/ACCESS Interface to ODBC",
    "section": "References:",
    "text": "References:\n\nSAS/ACCESS 9.2 for Relational Databases Reference, Fourth Edition. http://support.sas.com/documentation/cdl/en/acreldb/63647/HTML/default/viewer.htm#titlepage.htm\n\nSAS 9.2 SQL Procedure User’s Guide. http://support.sas.com/documentation/cdl/en/sqlproc/62086/HTML/default/viewer.htm#titlepage.htm"
  },
  {
    "objectID": "posts/13-questionstoaskentrepreneurs/index.html",
    "href": "posts/13-questionstoaskentrepreneurs/index.html",
    "title": "Some questions to ask entrepreneurs",
    "section": "",
    "text": "Based on MITx bootcamp notes\n\nPart 1 - Basic questions:\nQ01. What is the problem you want to solve?\nQ02. Who experiences the problem?\nQ03. How do you want to solve this problem?\nQ04. Why is this a better solution?\nQ05. If you could describe your product in about 10 words without using anything fancy. How would you say it?\nQ06. What is the one thing you feel you can do (for your customer) better than everyone else?\n\n\nPart 2 - Business Plan:\nQ07. Why did you pick up this particular field of all the other things to solve?\nQ08. What are the many business opportunities do you see in this field?\nQ09. What skills do you need to learn to pursue these opportunities?\nQ10. And with people of what skills, strengths, and interests would you like to collaborate in that pursuit?\n\n\nPart 3 - Market Segmentation:\nQ11. What are you top 3 markets (e.g. educational, aerospace, medical etc… ) for your business?\nQ12. Did you do primary customer research? If so, what was the main feedback given?\nQ13. In what way did you change your business model as a result of this feedback?\n\n\nPart 4 - Beachhead market: initial market segment that is easy to grow & profit\nQ14. What do you think is your beachhead market? (e.g. small biz.,schools, houses, retail etc…)\nQ15. Is there competition that could block you from getting this business of this market?\nQ16. What is your plan to deal with them?\nQ17. If you win this market will it help you to win other market segments?\n\n\nPart 5 - End user profile: Common characteristics among all your customers\nQ18. Have you profiled your customers in terms of demographics, their motivations for solving their problem & also socio-economic profiles?\nQ19. Do you have the unique characteristics of your customers that you can use to identify other customers?\nQ20. How many such kind of customers are there in your beachhead market?\n\n\nPart 6 - TAM (Total addressable market): Total revenue from your beachhead market.\nTAM = Total no. of customers possible * Avg. Revenue per customer per Year.\nQ21. Do you have any idea of your TAM size? Can you capture 100% of it?\nQ22. How much percent can you achieve in the next 5 years?\n\n\nPart 7 - Persona: More detailed profiling of customers.\nQ23. How do you plan to get your next 100 customers?\n\n\nPart 8 - High Level Product Specification: Complete overview of the product.\nQ24. Does your team & customers have the same idea about what the service is & the direction it is evolving?\nQ25. What steps are you taking to improve on this?\n\n\nPart 9 - Last Words:\nQ26. What motivates you to do this every single day?\nQ27. What keeps you awake at night about your business?\nQ28. Is there any situation where you might seriously consider walking away?\nQ29. What are the top 3 things you learnt from starting & running this?\nQ30. Anything you wish to share about your experiences that most people don’t know?\n\n\nPart 10 - Introspection\n\nWhat made you interested to meet this entrepreneur and how the entrepreneur’s work or life story aligns with your interests.\nWhat you learned about the entrepreneur and the entrepreneur’s startup before your meeting, and what questions you prepared for the entrepreneur.\nWhat you learned by meeting the entrepreneur, and how that compares or contrasts with your perspective before the meeting.\nWhat problem the entrepreneur is solving, for what target customer, with what solution, and what makes the solution unique.\nDescribe your thoughts on the potential of the entrepreneur’s startup."
  },
  {
    "objectID": "posts/14-sqldbadmin/index.html",
    "href": "posts/14-sqldbadmin/index.html",
    "title": "SQL & Database Administration Setup & Tips",
    "section": "",
    "text": "r.bat File to read an sql script from the command line & display the content back to the CLI.\nusage: r.bat script_name.sql\necho off\nset arg1=%1\nset arg2=%~n1\nshift\nshift\n\nrem mysql -h &lt;hostname&gt; -u &lt;username&gt; -p&lt;pass&gt; &lt;dbname&gt; &lt; %arg1%\nmysql -h &lt;hostname&gt; -u &lt;username&gt; -p&lt;pass&gt; &lt;dbname&gt; &lt; %arg1%\n\n\na.bat File to read an sql script from the command line & display the content back to a text file.\nusage: a.bat script_name.sql\necho off\nset arg1=%1\nset arg2=%~n1\nshift\nshift\n\nmysql -h &lt;hostname&gt; -u &lt;username&gt; -p&lt;pass&gt; &lt;dbname&gt; &lt; %arg1% &gt; %arg2%.txt\nmore %arg2%.txt\n\n\nrunserver.bat add path and start mysql client.\nusage: runserver.bat\n\n\n\nrunserver.bat add path and start mysql client.\nusage: runserver.bat\necho off\nset path=C:\\mysql-5.1-winx64\\bin;%path%\nrem sudo ifup enp0s8\nmysql -h &lt;hostname&gt; -u &lt;username&gt; -p&lt;pass&gt; &lt;dbname&gt;"
  },
  {
    "objectID": "posts/cs01-suitabilityofstationarydemandmodels/index.html",
    "href": "posts/cs01-suitabilityofstationarydemandmodels/index.html",
    "title": "Suitability of stationary demand models for forecasting",
    "section": "",
    "text": "Objective\nStudy the suitability of stationary demand models for forecasting sales at the Shah Alam Palm Oil Company.\n\n\nIntroduction\nPalm oil is harvested from the fruit of oil palm trees and is widely used as a cooking oil throughout Africa, Southeast Asia, and parts of Brazil. It is becoming widely used throughout the world as it is a lower cost alternative to other vegetable oils and has other attractive properties.\nThe Shah Alam Palm Oil Company (SAPOC) harvests, processes, and sells palm oil throughout the region. As a demand analyst, you are asked to review the sales volume (in pounds) of you premium palm oil by one of your customers, a local grocery store in the region.\n\n\nVisualization of the raw data\n\nQ1. What is the trend over the last three years?\nThere appears to be a positive trend. From the graph there is an increase in the demand of palm oil by about 24 lbs per month for the last three years.\n\nQ2. Does there appear to be any seasonality in the demand pattern?\nYes. If we plot the data by the months for each of the years, there seems to be seasonality to the demand. Demand is low from January to May. It picks up from June to August and then again from October to December.\n\nQ3. What is the forecast for demand in January 2015?\n\n\n\n\n\n\n\n\nMethod\nForecast for Jan 2015 (lbs)\nActual value in Dec 2014\n\n\n\n\nNaïve Model\n1512\n1512\n\n\nCumulative Model\n957.9444\n1512\n\n\n12 Period Moving Average\n1173.667\n1512\n\n\n\nQ4. What is the root mean square error (RMSE) for a next period forecast for these three years of demand?\n\n\n\nMethod\nRMSE\n\n\n\n\n\nNaïve Model\n383.7282\n\n\n\nCumulative\n419.8851\n\n\n\n12 Period Moving Average\n423.33\n\n\n\n\nQ5. Which of these three models is most appropriate for forecasting the January 2015 demand?\nNone. As shown above, the palm oil data shows a positive trend & seasonality during the years. The Naïve model forecast for Jan 2015 is clearly different from the previous trends. While the cumulative & naïve models are quite calm indicating they are forecasting demand closer to the average of the data. The main reason for this discrepancies is that the above three models assume a stationary demand that is very close to the level of the mean."
  },
  {
    "objectID": "posts/cs02-performancecharacteristics/index.html",
    "href": "posts/cs02-performancecharacteristics/index.html",
    "title": "Performance characteristics of forecasting models",
    "section": "",
    "text": "Objective\nInvestigate existing forecasting capabilities of Ordroid devices & provide suggestions.\n\nIntroduction\nYou have just been hired by a company that manufactures mid‐range communication devices that use the Ordroid open source operating system. The company is focused on innovating its products and has not put much thought on its inventory or forecasting capabilities. Your boss thinks there might be a problem in the forecasting of the Ordroid Devices and wants you to figure it out. The Ordroid, far from being new to the market, has been out for two years.\nKnowing this, you have asked for data on both years of historical sales as well as any forecasts, promotions, pricing changes, or competitive analyses made during this time. Your boss laughs and provides you with all the data they have: the last six months of sales. You ask to meet with the current demand planner for the Ordroid Devices and she tells you that they use a forecasting algorithm of her own design and there is no documentation.\n\n\nVisualization of the raw data\nRaw data & forecasts supplied by the demand planner at Ordroid Devices\n\nCalculate some different performance characteristics for the data sample given.\n\\[\nMD=\\frac{\\sum_{i=1}^{n} (Actual_{i}-Forecast_{i})}{n}\n\\] \\[\nMAD=\\frac{\\sum_{i=1}^{n} \\lvert Actual_{i}-Forecast_{i}\\rvert}{n}\n\\] \\[\nRMSE=\\sqrt{\\frac{\\sum_{i=1}^{n} (Actual_{i}-Forecast_{i})^2}{n}}\n\\] \\[\nMPE=\\frac{\\sum_{i=1}^{n} \\frac{ (Actual_{i}-Forecast_{i})}{Actual_{i}}}{n}\n\\] \\[\nMAPE=\\frac{\\sum_{i=1}^{n} \\lvert \\frac{Actual_{i}-Forecast_{i}}{Actual_{i}}\\rvert}{n}\n\\]\n\n\n\n\n\n\n\nNumber of devices\n\n\n\n\nMean Deviation\nMD\n112.5\n\n\nMean Absolute Deviation\nMAD\n509.5\n\n\nRoot Mean Square Error\nRMSE\n540.6115\n\n\nMean Percent Error\nMPE\n0.04112\n\n\nMean Absolute Percent Error\nMAPE\n0.269002\n\n\n\n\nQ1. What can you say about the presence seasonality of demand?\nSeasonality requires a whole cycle. There is not even one full year of data. So as of now, it is too early to fully evaluate seasonality. You need at least two full cycles to determine seasonality.\nQ2. What can you say about the presence of a trend in the demand?\nAlthough we don’t have a year’s worth of data, there seems to be a positive trend of about 10% increase in demand in the data or about 171 devices per month.\n\nQ3. What can you say about the bias of the forecast?\nA bias is a persistent tendency to over or under predict. These forecasts are not persistent in either. In fact, of the six periods, half are over forecast and half are under forecast. So, there does not appear to be any bias in the forecast.\nQ4. What can you say about the accuracy of the forecast?\nThis is not a very good forecast because even though there is a strong positive trend, the forecasts ignores the trend & also the MAPE is almost 27% ‐ quite high."
  },
  {
    "objectID": "posts/cs03-comparisonnaivecumulative/index.html",
    "href": "posts/cs03-comparisonnaivecumulative/index.html",
    "title": "Improving the naïve model forecast using cumulative period model",
    "section": "",
    "text": "Objective:\nComparing error metrics by switching naïve model to cumulative model.\nIntroduction:\nYou have been hired by General Miles, a company that produces healthy gluten‐free breakfast cereal bars. The last market introduction happened a year ago and your manager thinks there might be an issue in the forecasting methodology. They are currently using a simple Naive forecasting model and you think there might be some room for improvement.\nYour boss provides you with the sales for the last 12 months and the forecasts for the last 11 months. No data is available to forecast the first month as the product was totally new to the market at the time.\nVisualization of the raw data\n\nComparison of the forecasting models\n\nComparison of Model Error Metrics\n\n\n\nError Metric\nRMSE\nMAPE\n\n\n\n\nNaïve Model\n80.45665\n0.155729\n\n\nCumulative Model\n131.226\n0.269029\n\n\n\nConclusion:\nThe cumulative model in this case is worse than the Naïve model since the RMSE and the MAPE values are greater. It does not react quickly enough to adapt to the high variability in sales for this new gluten free cereal bar."
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "",
    "text": "To help the Fenway Park concessions evaluate and compare the quality of these three competing forecasting approaches."
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html#visualization-of-raw-data",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html#visualization-of-raw-data",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "Visualization of raw data",
    "text": "Visualization of raw data"
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html#comparison-of-the-mean-deviation-md-among-the-three-models",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html#comparison-of-the-mean-deviation-md-among-the-three-models",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "Comparison of the mean deviation (MD) among the three models",
    "text": "Comparison of the mean deviation (MD) among the three models\n\n\n\n\n\n\n\n\nModel\nMD\n% deviation from mean of actual sales (3750.056)\n\n\n\n\nM1\n‐526.988\n‐14.0528\n\n\nM2\n24.45679\n0.652171\n\n\nM3\n‐11.6852\n‐0.3116\n\n\n\nAs seen above M1 exhibits the most bias. M3 has the least bias. This is done to\nModels 2 and 3 have an average error in the vicinity of only a few dozen hotdogs from the actual. In fact,\nM2 & M3 have a MD equivalent to less than 1% of the average hot dog sales per game.\nHowever, Model 1 seems to consistently over‐estimate the demand for hotdogs by an average of 500 units per game ‐ or about 14% of the average hot dog sales per game). Therefore, we can say that – based on the historical data we have – Models 2 and 3 are less biased than Model 1. Model 1 is the most biased of the pack and it is OVER forecasting."
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html#calculation-of-rmse-of-each-of-the-models",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html#calculation-of-rmse-of-each-of-the-models",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "Calculation of RMSE of each of the models",
    "text": "Calculation of RMSE of each of the models\n\n\n\nModel\nRMSE\n\n\n\n\nM1\n597.6846\n\n\nM2\n286.457\n\n\nM3\n500.2935\n\n\n\nAs seen above, M2 has an error that is half of the other models & hence is the most suitable forecasting model besides also have a small deviation in forecasted sales from the actual demand."
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html#effect-of-underestimating-overestimating-by-the-models",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html#effect-of-underestimating-overestimating-by-the-models",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "Effect of underestimating & overestimating by the models",
    "text": "Effect of underestimating & overestimating by the models\nAs mentioned in the introduction,\n\n\n\n\n\n\n\n\nForecasting Issues\nEffect\nCost\n\n\n\n\nUnderestimating\nUnsatisfied & hungry fans\n$3 per lost sale\n\n\nOverestimating\nWaste of ingredients, energy & labor\n$2 per unsold hot dog\n\n\n\nCalculations of the lost‐sale or unsold inventory by the three models.\n\n\n\nMetric\nM1\nM2\nM3\n\n\n\n\nShortage of hotdogs\n0\n21765\n32883\n\n\nCost of Lost Sales $\n0\n65295\n98649\n\n\nUnsold Hotdogs\n85372\n17803\n34776\n\n\nCost of Unsold hotdogs $\n170744\n35606\n69552\n\n\nTotal Loss $\n170744\n100901\n168201\n\n\n\n\nLeast Total loss"
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html#conclusion",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html#conclusion",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "Conclusion",
    "text": "Conclusion\nWhen comparing the MD, RMSE among the three models, M2 seems to be the most accurate in its predictions and it also manages to provide the least total loss & a balance between the cost of lost sales and unsold hot dogs better than the other two models.\nTherefore, from among the three available options, M2 seems to make the predictions that make most economic sense."
  },
  {
    "objectID": "posts/cs05-evaluationofstationarydemand/index.html",
    "href": "posts/cs05-evaluationofstationarydemand/index.html",
    "title": "Evaluation of stationary demand models",
    "section": "",
    "text": "Objective\nSelect a suitable model among the given choices."
  },
  {
    "objectID": "posts/cs05-evaluationofstationarydemand/index.html#check-for-stationary-demand",
    "href": "posts/cs05-evaluationofstationarydemand/index.html#check-for-stationary-demand",
    "title": "Evaluation of stationary demand models",
    "section": "Check for stationary demand",
    "text": "Check for stationary demand\nOne way of doing is to determine the coefficient of variation (CV)\n\\[\nCV=\\frac{STDEV(data)}{AVERAGE(data)}\n\\]\nWe get\n\n\n\nSTDEV\n62.6899312\n\n\n\n\nAVERAGE\n1103.78571\n\n\nCV\n0.05679538\n\n\n\nCV is very low & hence the demand is quite stationary & stable in nature."
  },
  {
    "objectID": "posts/cs05-evaluationofstationarydemand/index.html#calculations-among-the-models",
    "href": "posts/cs05-evaluationofstationarydemand/index.html#calculations-among-the-models",
    "title": "Evaluation of stationary demand models",
    "section": "Calculations among the models",
    "text": "Calculations among the models\n\n\n\n\nPrediction for period 15\nMAPE(%)\n\n\n\n\nPeriod 14 value\n1169\n\n\n\nNaïve\n1169\n7.08446274\n\n\nCumulative\n1103.78571\n5.13422811\n\n\n2MA\n1145\n6.46420824\n\n\n4MA\n1113.25\n4.79251584\n\n\n\nHere we see that the moving average forecasts need not always be between the naïve & cumulative forecasts."
  },
  {
    "objectID": "posts/cs05-evaluationofstationarydemand/index.html#selection-of-the-model-in-the-presence-of-a-trend",
    "href": "posts/cs05-evaluationofstationarydemand/index.html#selection-of-the-model-in-the-presence-of-a-trend",
    "title": "Evaluation of stationary demand models",
    "section": "Selection of the model in the presence of a trend",
    "text": "Selection of the model in the presence of a trend\n\nIf we assume there is a positive trend in the data then none of these models are appropriate for demand with a trend pattern. The Cumulative, Naive, and Moving Average forecasts all assume stationary demand. That means that you only assume a Level pattern to the demand with some random noise."
  },
  {
    "objectID": "posts/cs06-choiceofmovingaverageorexponential/index.html",
    "href": "posts/cs06-choiceofmovingaverageorexponential/index.html",
    "title": "Choice of moving average or exponential smoothing for a particular product profile",
    "section": "",
    "text": "The demand data for a product has been shown in the table below. Compare the forecasts using a Moving Average with a period of 5 months, MA(5), and an Exponential smoothing Method with an α of 0.33. For Exponential Smoothing use the midpoint of first 5 month range of the average as the initial Forecast. (Hint: the Exponential Smoothing Forecast will be initialized with a forecast of 4951 for April made in March.)"
  },
  {
    "objectID": "posts/cs06-choiceofmovingaverageorexponential/index.html#introduction",
    "href": "posts/cs06-choiceofmovingaverageorexponential/index.html#introduction",
    "title": "Choice of moving average or exponential smoothing for a particular product profile",
    "section": "",
    "text": "The demand data for a product has been shown in the table below. Compare the forecasts using a Moving Average with a period of 5 months, MA(5), and an Exponential smoothing Method with an α of 0.33. For Exponential Smoothing use the midpoint of first 5 month range of the average as the initial Forecast. (Hint: the Exponential Smoothing Forecast will be initialized with a forecast of 4951 for April made in March.)"
  },
  {
    "objectID": "posts/cs06-choiceofmovingaverageorexponential/index.html#visualization-of-raw-data",
    "href": "posts/cs06-choiceofmovingaverageorexponential/index.html#visualization-of-raw-data",
    "title": "Choice of moving average or exponential smoothing for a particular product profile",
    "section": "Visualization of raw data",
    "text": "Visualization of raw data\n\n\nForecasting with 5-point moving average & simple exponential smoothing\nHere we use α=1/3 for our exponential smoothing model\n\n\n\nAccuracy of the models\nTo estimate the accuracy of the models, we first compare the Mean Absolute Deviation (MAD) for each of the models.\n\n\n\n\n5MA\nSES\n\n\n\n\nMAD\n453.7429\n432.0553\n\n\n\nAlso comparing the error cumulatively gives a better picture of the accuracy of each of the models.\n\n\n\nConclusion\nFor this particular product, the forecasts from June to December show that SES is performing better than 5MA. SES outperforms 5MA for 5 months while 5MA outperforms SES for only 2 months."
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "",
    "text": "Objective\nFormulation & testing of different exponential models on the product data."
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html#simple-exponential-smoothing-ses",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html#simple-exponential-smoothing-ses",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "Simple Exponential Smoothing (SES)",
    "text": "Simple Exponential Smoothing (SES)\nWe know that SES assumes stationary demand. i.e. it forecasts does not take into account trends or seasonalities. Even so, we would still like to know the effect of using SES on the forecasts.\nUnderlying model: \\(x_{t} = a + e_{t}\\)\nForecasting model: \\(\\hat{x}_{t,t+1} = \\alpha x_{t} + (1 – \\alpha) \\hat{x}_{t-1,t}\\)\nWhere \\(\\hat{x}_{t,t+1}\\) is forecast for the next period, \\(x_{t}\\) is the present actual demand and \\(\\hat{x}_{t-1,t}\\) is forecast for the previous period.\nInitialization of the parameters\nThere are many ways of doing this. We can take the centered average for the first 4 or 5 periods. We can also take the average of the first 3, 4 or 5 periods depending on the data.\nWe take \\(\\hat{a}_{4,5}\\)=205.25\nWe take α=0.15\nUsing the above model, the forecast for period 25 is around 654 bars. Also the forecast for period\n30 will also be the same i.e. 654 since the model assumes stationary demand.\nMAPE for SES is 0.279329"
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html#holts-model-hm",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html#holts-model-hm",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "Holt’s Model (HM)",
    "text": "Holt’s Model (HM)\n\nSince the data shows a positive trend, we use HM which assumes level & trend.\nUnderlying model: \\(x_{t} = a + bt + e_{t}\\)\nForecasting model: \\(\\hat{x}_{t,t+\\tau} = \\hat{a}_{t}+\\tau \\hat{b}_{t}\\)\nUpdating Component:\n\\(\\hat{a}_{t}=\\alpha \\hat{x}_{t}+ (1-\\alpha)\\hat{x}_{t-1,t}\\)\na^t = α xt + (1 – α) x^t‐1,t\n\\(\\hat{b}_{t}=\\beta (\\hat{a}_{t}-\\hat{a}_{t-1})+(1-\\beta) \\hat{b}_{t-1}\\)\nb^t = β (a^t ‐ a^t‐1) + (1 – β) b^t‐1\nInitialization of parameters\nWe take α = 0.2 and β = 0.05\nWe take \\(\\hat{a}_{t}\\) = 157.5 & \\(\\hat{b}_{t}\\) = 19.1"
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html#comparison-of-models",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html#comparison-of-models",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "Comparison of models",
    "text": "Comparison of models"
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html#mape-of-the-various-models",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html#mape-of-the-various-models",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "MAPE of the various models",
    "text": "MAPE of the various models\n\n\n\n\nMAPE\n\n\n\n\nSES\n0.279329\n\n\nHM (alp=0.2bet=0.05)\n0.128729\n\n\nHM (alp=0.5bet=0.05)\n0.120443\n\n\nHM (alp=0.99bet=0.05)\n0.112559\n\n\n\nThe MAPE and various other measures such as RMSE or MAD or most any other metric will improve as we increase the value of Alpha. This does not mean we are getting a better model. This means is that we are only fitting the model better to the historical data that we have. We are simple placing more weight to the most recent observations. Therefore, we should monitor effect of alpha & change as and when required."
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html#conclusion",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html#conclusion",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "Conclusion",
    "text": "Conclusion\nHolt’s model with alpha=0.2 seems to be best way to forecast the demand of sugar cereals."
  },
  {
    "objectID": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html",
    "href": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html",
    "title": "Exponential smoothing models at TrainMax Systems",
    "section": "",
    "text": "Objective:"
  },
  {
    "objectID": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#simple-exponential-smoothing-ses-model",
    "href": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#simple-exponential-smoothing-ses-model",
    "title": "Exponential smoothing models at TrainMax Systems",
    "section": "Simple Exponential Smoothing (SES) model",
    "text": "Simple Exponential Smoothing (SES) model\nWe try this model as it looks like there is stationary demand & no trend. We need to assign the initial parameters first. We start with period 0 where we assume the forecast for the period 1 is the same as the demand for period 1. Also assume initial α=0.12\nUsing this we have"
  },
  {
    "objectID": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#varying-alpha-to-get-the-most-accurate-model",
    "href": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#varying-alpha-to-get-the-most-accurate-model",
    "title": "Exponential smoothing models at TrainMax Systems",
    "section": "Varying alpha to get the most accurate model",
    "text": "Varying alpha to get the most accurate model\n\n\n\n\n\n\n\n\n\n\n\n\nSES alpha\nMAPE\n\nRMSE\n\n\n\n\n\n\n\n0.12\n0.052619\n\n74.00847195\n\n\n\n\n\n0.20925\n0.04772\nLEAST MAPE\n69.79034\n\n\n\n\n\n0.29816\n0.048748836\n\n68.96512784\nLEAST RMSE\n\n\n\n\n0.4\n0.05095862\n\n69.57546188\n\n\n\n\n\n0.9\n0.062207335\n\n81.68324857"
  },
  {
    "objectID": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#conclusion",
    "href": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#conclusion",
    "title": "Exponential smoothing models at TrainMax Systems",
    "section": "Conclusion",
    "text": "Conclusion\nBy varying Alpha we are merely trying to fit the model & minimize the error to historical data. Such tweaking will not necessarily produce a forecast. Also the coefficient of variation needs to be looked at. Higher CV means that data is more volatile & thus Alpha needs to be high to follow these fast changes.\nAlso increasing Alpha does not change the forecast much. This shows the robustness of the SES model.\nFrom the above data, an increase between 0.15 and 0.20 would give a good forecasting model. But whatever the value of Alpha to be used in the model, it needs to be tested on new data to see how it performs."
  },
  {
    "objectID": "posts/cs09-forecastingseasonaltrendsblayk/index.html",
    "href": "posts/cs09-forecastingseasonaltrendsblayk/index.html",
    "title": "Forecasting with seasonal trends at BLAYK restaurant",
    "section": "",
    "text": "Objective"
  },
  {
    "objectID": "posts/cs09-forecastingseasonaltrendsblayk/index.html#initial-seasonality-factors",
    "href": "posts/cs09-forecastingseasonaltrendsblayk/index.html#initial-seasonality-factors",
    "title": "Forecasting with seasonal trends at BLAYK restaurant",
    "section": "Initial Seasonality Factors",
    "text": "Initial Seasonality Factors\nThe above diagram clearly points to a seasonality of the sales.\nSince at this point it is not very clear as to whether there is a trend in the data or not, we find use two methods to find the seasonality factors.\n\nAssuming no trend\nWith no trend the seasonality factors (SF) need not be normalized each season.\nSF per period = total sales per shift / (total sales per month/no. of periods)\nAlso\nSF per period = total sales per shift / average no. of sales per period\nMathematically we can express it as\n\\[\nF_{t}=\\frac{\\sum_{t=1}^{n} D}{(\\sum_{t=1}^{n} D_{t})/P}\n\\]\n\n\nCentered Moving Average Method (CMA)\nSince each season has 4 periods, we use 4‐point Centered Moving Average. Here since the season has an even number of points. We need to take the moving average of the season from both sides & then take the final average.\nBelow is a sample of the data used to calculate part of the Fi’s\nMATop is the average of Shift 1,2,3 & 4\nMABottom is the average of Shift 2,3,4 & 5\nMA_Avgi is the average of MATop & MABottom.\nEach Fi is the xi/MA_Avgi except the first two & last two of the time series. The first two & last two Fi are calculated by first & the last MA_Avg values respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime Period d(t)\nDate\nShift Number\nPints Sold, xi\nMATop\nMABottom\nMA_Avg i\nFi\n\n\n\n\n1\n1‐Jun\n1\n357\n\n\n\n\n\n\n2\n1‐Jun\n2\n49\n\n\n\n\n\n\n3\n1‐Jun\n3\n242\n260\n264\n262\n\n\n\n4\n1‐Jun\n4\n391\n264\n264\n264\n\n\n\n5\n2‐Jun\n1\n373\n264\n264\n264\n\n\n\n6\n2‐Jun\n2\n50\n264\n269\n266\n\n\n\n7\n2‐Jun\n3\n243\n269\n269\n269\n\n\n\n8\n2‐Jun\n4\n408\n269\n269\n269\n\n\n\n\nNow if the assumption is incorrect & then is a small trend, then the sum of the factors will not add up to number of periods in a season. i.e P = 4 Hence a correction is required in the form and we simply multiply each of your Seasonality Factors by\n\\[\n\\frac{P}{\\sum_{i=1}^{n} F_{i}}\n\\]\nOnce all the Fi are calculated, we average them according to Shift Number. The summary is in the table.\n\n\n\n\n\n\n\n\n\n\n\nTotal Pints Sold\nIf equal sales per shift, pints per shift sold\nRatio of Sales per shift compared with average\n4‐point Moving Centered Averaged Seasonality Factors\n\n\n\n\nEntire Month\n37423\n\n\n\n\n\nShift 1\n13045\n9356\n1.39432969\n1.402007\n\n\nShift 2\n1737\n9356\n0.185661224\n0.185922\n\n\nShift 3\n8700\n9356\n0.929909414\n0.928191\n\n\nShift 4\n13941\n9356\n1.490099671\n1.483154\n\n\n\n\n\n4\n3.999273"
  },
  {
    "objectID": "posts/cs09-forecastingseasonaltrendsblayk/index.html#holtwinter-model-levelseasonalitytrend",
    "href": "posts/cs09-forecastingseasonaltrendsblayk/index.html#holtwinter-model-levelseasonalitytrend",
    "title": "Forecasting with seasonal trends at BLAYK restaurant",
    "section": "Holt­Winter Model (level+seasonality+trend)",
    "text": "Holt­Winter Model (level+seasonality+trend)\nLevel & Trend:\nRunning a linear regression we get the equation.\ny = 0.812x + 262.6\nFrom the regression equation we get a level of about 263 pints of beer per shift with an trend of 0.8 additional pints per time period. i.e.\nThe regression gives you an estimated level of 265 pints per each shift with a trend of 0.80 additional pints per time period. This means that the sales of beer is increasing about 3.2 pints per day. Hence there is a positive trend trend.\n\n\nSeasonality\nThis involves estimating the initial values of the level and trend “de‐seasoning” the actual demand by the Seasonality Factors we just found. Part of the data used to calculate the normalized seasonality factors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime Period (t)\nDate\nShift Number\nPints Sold\nMATop\nMABottom\nMA_Avg\nFi\nSUM of each season\nFi Normalized\nNormalized Sum\n—\n\n\n\n\n1\n1‐Jun\n1\n357\n\n\n\n1.363897\n3.957406897\n1.378576309\n4\n\n\n\n2\n1‐Jun\n2\n49\n\n\n\n0.187202\n3.957406897\n0.189216356\n\n\n\n\n3\n1‐Jun\n3\n242\n260\n264\n262\n0.924546\n3.957406897\n0.934497106\n\n\n\n\n4\n1‐Jun\n4\n391\n264\n264\n264\n1.481762\n3.957406897\n1.497710229\n\n\n\n\n5\n2‐Jun\n1\n373\n264\n264\n264\n1.41221\n4.023368199\n1.404007844\n4\n\n\n\n6\n2‐Jun\n2\n50\n264\n269\n266\n0.187705\n4.023368199\n0.186615088\n\n\n\n\n7\n2‐Jun\n3\n243\n269\n269\n269\n0.904607\n4.023368199\n0.89935273\n\n\n\n\n8\n2‐Jun\n4\n408\n269\n269\n269\n1.518846\n4.023368199\n1.510024337\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo taking the average of the all the normalized factors we get,\n\n\n\n\nBefore Normalized\nAfter Normalized\n\n\n\n\n\n\nFs1\n1.402007\n1.402205906\n\n\n\n\nFs2\n0.185922\n0.185977102\n\n\n\n\nFs3\n0.928191\n0.928395676\n\n\n\n\nFs4\n1.483154\n1.483421316\n\n\n\n\nSUM\n3.999273\n4\n\n\n\n\n\n\n\nInitial Parameters\nAssume that Alpha=0.15, Beta=0.06 & gamma = 0.05\n\\[\n\\hat{x}_{t,t+\\tau}=(\\hat{a}_{t}+\\tau\\hat{b}_{t})\\hat{F}_{t+\\tau-P}\n\\] \\[\n\\hat{a}_{t}=\\alpha\\left(\\frac{x_{t}}{\\hat{F}_{t-P}}\\right)+(1-\\alpha)(\\hat{a}_{t-1}+\\hat{b}_{t-1})\n\\] \\[\n\\hat{b}_{t}=\\beta(\\hat{a}_{t}-\\hat{a}_{t-1})+(1-\\beta)\\hat{b}_{t-1}\n\\] \\[\n\\hat{F}_{t}=\\gamma \\left(\\frac{x_{t}}{\\hat{a}_{t}}\\right)+(1-\\gamma)\\hat{F}_{t-P}\n\\]\nWe have for the period 120, the following initial parameters,\n\n\n\nFs1\n1.402205906\n\n\n\nFs2\n0.185977102\n\n\n\nFs3\n0.928395676\n\n\n\nFs4\n1.483421316\n\n\n\n\\(\\hat{a}_{120}\\)\n360.04\n0.812*(120) + 262.6\n\n\n\\(\\hat{a}_{120}\\)\n0.812\n\n\n\nAlpha\n0.15\n\n\n\nBeta\n0.06\n\n\n\nGamma\n0.05\n\n\n\n\nUsing above data we can start forecasting for the coming periods 122 i.e. July 2 Shift 2\n\n\n\n\n\n\n\n\n\n\n\n\nActual x(t)\n\\(\\hat{a}_{i}\\)\n\\(\\hat{b}_{i}\\)\n\\(\\hat{F}_{i}\\)\n\\(\\hat{x}_{t+4}\\)\n\n\n\n\n120\n557\n360\n0.81\n\n\n\n\n121\n520\n362.3151378\n0.900308265\n1.403856344\n513.694 (for t=125)\n\n\n\nUsing just the 122 forecast, the rest of the periods i.e. 123, 124 & 125 can be calculated using\n\\[\n\\hat{x}_{t,t+\\tau}=(\\hat{a}_{t}+\\tau\\hat{b}_{t}) \\hat{F}_{t+\\tau-P}\n\\]"
  },
  {
    "objectID": "posts/cs09-forecastingseasonaltrendsblayk/index.html#conclusion",
    "href": "posts/cs09-forecastingseasonaltrendsblayk/index.html#conclusion",
    "title": "Forecasting with seasonal trends at BLAYK restaurant",
    "section": "Conclusion",
    "text": "Conclusion\nAs observed above, the data about beer consumption follows seasonality & has a positive trend. This can be modeled using the Holt‐Winter Model. Of course, error analysis must be done to tweak the model especially the seasonality factors."
  },
  {
    "objectID": "posts/cs10-extractinginitialparametersexistingholtwinter/index.html",
    "href": "posts/cs10-extractinginitialparametersexistingholtwinter/index.html",
    "title": "Extracting initial parameters from an existing Holt‐Winter forecasting model",
    "section": "",
    "text": "Objective\nThe model is known but the initial parameters need to be found.\n\nIntroduction\nYou are hired by a local company to help them improve their forecasting capabilities. You are tasked with coming up with quarterly forecasts for an item that appears to have level, seasonality, and trend. The good news is that the company has an existing Holt‐Winter forecasting model. The bad news is that no one knows what the parameters (Alpha, Beta, or Gamma) are.\nYou do have some information. For example, you know that historically, the demand in each quarter follows this distribution:\n\nQ1 (January through March) = 50% of average quarterly demand\nQ2 (April through June) = 75% of average quarterly demand\nQ3 (July through September) = 150% of average quarterly demand\nQ4 (October through December) = 125% of average quarterly demand.\n\nYou just ran the forecast at the end of September (end of 2014Q3) and you have the following estimates:\nFor level: \\(\\hat{a}_{2014Q3}\\) = 1052 units\nFor trend: \\(\\hat{b}_{2014Q3}\\) = 46.2 units per quarter\nQ1. What is the forecast for demand for 2014Q4?\nWe know that\n\\(\\hat{x}_{t,t+\\tau}=(\\hat{a_{t}}+\\tau\\hat{b_{t}})\\hat{F}_{t+\\tau-P}\\)\n\\(\\hat{a}_{t}=\\alpha \\left(\\frac{x_{t}}{\\hat{F}_{t-P}}\\right) +(1-\\alpha)(\\hat{a}_{t-1}+\\hat{b}_{t-1})\\)\n\\(\\hat{b}_{t}=\\beta(\\hat{a}_{t}-\\hat{a}_{t-1})+(1-\\beta)\\hat{b}_{t-1}\\)\n\\(\\hat{F}_{t}=\\gamma \\left(\\frac{x_{t}}{\\hat{a}_{t}}\\right) +(1-\\gamma)\\hat{F}_{t-P}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nx\n\\(\\hat{a}\\)\n\\(\\hat{b}\\)\n\\(\\hat{F}\\)\n\\(\\hat{x}\\)\n\\(\\hat{F}_{t-P}\\)\n\n\n\n\n2014Q3\n\n1052\n46.2\n\n1372.75\n1.25 (2013Q4)\n\n\n2014Q4\n\n\n\n\n\n\n\n\n\nQ2. Suppose the actual demand in 2014Q4 is 1100 units. What is the smallest & largest possible value for your estimate for level, \\(\\hat{a}_{2014Q4}\\)?\nThe fourth quarter level estimate = \\(\\hat{a}_{2014Q4} = \\hat{x}_{2014Q4} / \\hat{F}_{2013Q4}\\)\nWithout seasonality, level estimate = \\(\\hat{a}_{2014Q4} = (\\hat{a}_{2014Q3}+\\hat{b}_{2014Q34})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlpha\n\nx\n\n\\(\\hat{a}\\)\n\\(\\hat{b}\\)\n\\(\\hat{F}\\)\n\\(\\hat{x}\\)\n\\(\\hat{a}_{t-P}\\)\n\n\n\n\n\n\n2014Q3\n\n\n1052\n46.2\n\n1372.75\n1.25 (2013Q4)\n\n\nSmallest\n1\n2014Q4\n\n1100\n880\n\n\n\n\n\n\nLargest\n0\n2014Q4\n\n1100\n1098.2\n\n\n\n\n\n\n\nQ3. The model was run at the end of 2014Q4. It provided you with the most recent estimates of each pattern. A) The estimate for level, \\(\\hat{a}_{2014Q4}\\) was 1065.5. What is value of alpha? B) Estimate of trend, \\(\\hat{b}_{2014Q4}\\) = 42.9, what is value of beta? C) Estimate of seasonality is \\(\\hat{F}_{2014Q4}\\) = 1.239\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nGiven\nEquation\n\n\nParameter\nSolved\n\n\n\n\n\\(\\hat{a}_{2014Q4}\\)\n1065.5\n1065.5\\(=\\alpha \\left(\\frac{1100}{1.25}\\right) +(1-\\alpha)(1052+46.2)\\)\n1100\n1052 46.2\nAlpha\n0.15\n\n\n\\(\\hat{b}_{2014Q4}\\)\n42.9\n42.9\\(=\\beta(1.065.5-1052)+(1-\\beta)46.2\\)\n1065.5 1052\n1 46.2\nBeta\n0.1\n\n\n\\(\\hat{F}_{2014Q4}\\)\n1.239\n1.239\\(=\\gamma \\left(\\frac{1100}{1065.5}\\right) +(1-\\gamma)1.25\\)\n1100\n1 1.25\nGamma\n0.05\n\n\n\nQ4. What is your forecast for demand for the 1st quarter of 2015? That is \\(\\hat{a}_{2014Q4,2015Q1}\\)?\n\\(\\hat{x}_{2014Q4,2015Q1} =(\\hat{a}_{2014Q1} + \\hat{b}_{2014Q1}) \\hat{F}_{2014Q4}\\)\nWe have the unnormalized seasonality factor, \\(\\hat{F}_{2014Q4} = 1.239\\)\nSince the sum of the most recent season estimates (0.500, 0.750, 1.500, and 1.239 for Q1, Q2, Q3, and\nQ4) adds up to 3.98912, , we need to normalize \\(\\hat{F}_{2014Q4}\\) before we use it in our calculations. We use the formula\n\\[\n\\hat{F}_{iadj}=\\hat{F}_{iold}\\frac{P}{\\sum{\\hat{F}_{i}}}\n\\]\nSo we have \\(\\hat{F}_{2014Q1} = 0.500*(4.000/3.989) = 0.50136\\)\n\\(\\hat{x}_{2014Q4,2015Q1} =(\\hat{a}_{2014Q1} + \\hat{b}_{2014Q1}) \\hat{F}_{2014Q1}\\)\n\\(\\hat{x}_{2014Q4,2015Q1} = (1065.5 + 42.9)(0.501) = 555.3084 = 555.31\\)\nIf you did not normalize the seasonality factor you would have gotten = (1065.5 + 42.9)(0.500) = 554.20. Normalizing the seasonality factors prevents the estimates from drifting. In this case, it is a small drift ‐ but over time it would grow."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/sql/index.html",
    "href": "posts/sql/index.html",
    "title": "SQL Guide.",
    "section": "",
    "text": "This is a long guide to mastering SQL."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]